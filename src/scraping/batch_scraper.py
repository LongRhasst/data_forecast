import asyncio
import json
import logging
from pathlib import Path
from tiki_data import TikiPlaywrightScraper

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('batch_scraper.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

async def run_batch_scraping():
    """Ch·∫°y thu th·∫≠p d·ªØ li·ªáu h√†ng lo·∫°t theo keywords"""
    
    # ƒê·ªçc file keywords
    keywords_file = Path(__file__).parent / 'search_keywork.json'
    with open(keywords_file, 'r', encoding='utf-8') as f:
        keywords_data = json.load(f)
    
    # C·∫•u h√¨nh cho t·ª´ng nh√≥m
    configs = {
        'phone': {'max_products': 50, 'sleep': 30},
        'clothing': {'max_products': 50, 'sleep': 30},
        'motorcycle': {'max_products': 10, 'sleep': 30},
        'laptop': {'max_products': 10, 'sleep': 30}
    }
    
    all_keywords = []
    
    # Parse keywords
    for item in keywords_data:
        # Phone keywords
        if 'phone' in item:
            for keyword in item['phone']:
                all_keywords.append({
                    'keyword': keyword,
                    'category': 'phone',
                    'max_products': configs['phone']['max_products'],
                    'sleep': configs['phone']['sleep']
                })
        
        # Clothing keywords
        if 'clothing' in item:
            for category, keywords in item['clothing'].items():
                for keyword in keywords:
                    all_keywords.append({
                        'keyword': keyword,
                        'category': 'clothing',
                        'max_products': configs['clothing']['max_products'],
                        'sleep': configs['clothing']['sleep']
                    })
        
        # Motorcycle keywords
        if 'motorcycle' in item:
            for keyword in item['motorcycle']:
                all_keywords.append({
                    'keyword': keyword,
                    'category': 'motorcycle',
                    'max_products': configs['motorcycle']['max_products'],
                    'sleep': configs['motorcycle']['sleep']
                })
        
        # Laptop keywords
        if 'laptop' in item:
            for keyword in item['laptop']:
                all_keywords.append({
                    'keyword': keyword,
                    'category': 'laptop',
                    'max_products': configs['laptop']['max_products'],
                    'sleep': configs['laptop']['sleep']
                })
    
    logging.info(f"üöÄ B·∫Øt ƒë·∫ßu thu th·∫≠p d·ªØ li·ªáu cho {len(all_keywords)} keywords")
    logging.info(f"üìä T·ªïng quan:")
    logging.info(f"   - Phone: {len([k for k in all_keywords if k['category'] == 'phone'])} keywords x 50 s·∫£n ph·∫©m")
    logging.info(f"   - Clothing: {len([k for k in all_keywords if k['category'] == 'clothing'])} keywords x 50 s·∫£n ph·∫©m")
    logging.info(f"   - Motorcycle: {len([k for k in all_keywords if k['category'] == 'motorcycle'])} keywords x 10 s·∫£n ph·∫©m")
    logging.info(f"   - Laptop: {len([k for k in all_keywords if k['category'] == 'laptop'])} keywords x 10 s·∫£n ph·∫©m")
    
    # Ch·∫°y scraping cho t·ª´ng keyword
    for idx, kw_info in enumerate(all_keywords, 1):
        keyword = kw_info['keyword']
        category = kw_info['category']
        max_products = kw_info['max_products']
        sleep_time = kw_info['sleep']
        
        logging.info(f"\n{'='*80}")
        logging.info(f"üì¶ [{idx}/{len(all_keywords)}] ƒêang thu th·∫≠p: '{keyword}' (Category: {category})")
        logging.info(f"   ‚îú‚îÄ S·ªë s·∫£n ph·∫©m: {max_products}")
        logging.info(f"   ‚îî‚îÄ Sleep sau khi ho√†n th√†nh: {sleep_time}s")
        logging.info(f"{'='*80}\n")
        
        try:
            # T·∫°o scraper v·ªõi c·∫•u h√¨nh ph√π h·ª£p
            scraper = TikiPlaywrightScraper(
                search_term=keyword,
                max_products=max_products,
                max_reviews=20,  # Gi·ªØ nguy√™n 20 reviews m·ªói s·∫£n ph·∫©m
                headless=True  # Ch·∫°y ·∫©n ƒë·ªÉ nhanh h∆°n
            )
            
            # Ch·∫°y scraper
            await scraper.scrape()
            
            logging.info(f"‚úÖ Ho√†n th√†nh thu th·∫≠p cho '{keyword}'")
            
        except Exception as e:
            logging.error(f"‚ùå L·ªói khi thu th·∫≠p '{keyword}': {e}")
            continue
        
        # Sleep gi·ªØa c√°c request
        if idx < len(all_keywords):
            logging.info(f"‚è≥ ƒêang ch·ªù {sleep_time} gi√¢y tr∆∞·ªõc khi thu th·∫≠p keyword ti·∫øp theo...")
            await asyncio.sleep(sleep_time)
    
    logging.info(f"\n{'='*80}")
    logging.info(f"üéâ HO√ÄN TH√ÄNH! ƒê√£ thu th·∫≠p xong {len(all_keywords)} keywords")
    logging.info(f"{'='*80}")

if __name__ == "__main__":
    asyncio.run(run_batch_scraping())
