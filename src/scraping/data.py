import asyncio
import json
import logging
import argparse
import re
import random
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
from tqdm.asyncio import tqdm

# Setup logging
import sys
# Force UTF-8 encoding for console output
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')
if sys.stderr.encoding != 'utf-8':
    sys.stderr.reconfigure(encoding='utf-8')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('shopee_scraper.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

class ShopeePlaywrightScraper:
    def __init__(self, search_term, max_products=10, max_reviews=30, headless=False):
        """
        Scraper s·ª≠ d·ª•ng Playwright ƒë·ªÉ l·∫•y d·ªØ li·ªáu t·ª´ Shopee.
        
        Args:
            search_term: T·ª´ kh√≥a t√¨m ki·∫øm
            max_products: S·ªë l∆∞·ª£ng s·∫£n ph·∫©m t·ªëi ƒëa
            max_reviews: S·ªë l∆∞·ª£ng review t·ªëi ƒëa cho m·ªói s·∫£n ph·∫©m
            headless: Ch·∫°y browser ·∫©n hay kh√¥ng
        """
        self.search_term = search_term
        self.max_products = max_products
        self.max_reviews = max_reviews
        self.headless = headless
        self.output_file = f"shopee_{re.sub(r'[^a-z0-9_]+', '', search_term.lower())}.json"
        self.products_data = []
        self.cookies_file = "shopee_cookies.json"
        self.state_file = "shopee_state.json"
        
    async def _save_cookies(self, context):
        """L∆∞u cookies v√† storage state ƒë·ªÉ duy tr√¨ ƒëƒÉng nh·∫≠p"""
        try:
            await context.storage_state(path=self.state_file)
            logging.info(f"ƒê√£ l∆∞u session v√†o {self.state_file}")
        except Exception as e:
            logging.warning(f"Kh√¥ng th·ªÉ l∆∞u session: {e}")
    
    async def _load_cookies(self):
        """Ki·ªÉm tra xem c√≥ file cookies/state ƒë√£ l∆∞u kh√¥ng"""
        return Path(self.state_file).exists()
    
    async def _human_like_delay(self, min_sec=1, max_sec=3):
        """T·∫°o delay ng·∫´u nhi√™n gi·ªëng ng∆∞·ªùi d√πng th·∫≠t"""
        delay = random.uniform(min_sec, max_sec)
        await asyncio.sleep(delay)
    
    async def _simulate_human_behavior(self, page):
        """M√¥ ph·ªèng h√†nh vi ng∆∞·ªùi d√πng: di chuy·ªÉn chu·ªôt, scroll t·ª± nhi√™n"""
        try:
            # Random mouse movements
            for _ in range(random.randint(2, 4)):
                x = random.randint(100, 1500)
                y = random.randint(100, 800)
                await page.mouse.move(x, y)
                await self._human_like_delay(0.2, 0.5)
            
            # Scroll t·ª± nhi√™n t·ª´ng ƒëo·∫°n nh·ªè
            viewport_height = page.viewport_size['height']
            current_scroll = 0
            scroll_steps = random.randint(3, 6)
            
            for _ in range(scroll_steps):
                scroll_amount = random.randint(200, 500)
                current_scroll += scroll_amount
                await page.evaluate(f'window.scrollTo({{top: {current_scroll}, behavior: "smooth"}})')
                await self._human_like_delay(0.5, 1.5)
            
            # Scroll back l√™n m·ªôt ch√∫t (gi·ªëng ng∆∞·ªùi xem l·∫°i)
            if random.random() > 0.5:
                await page.evaluate(f'window.scrollBy({{top: -{random.randint(100, 300)}, behavior: "smooth"}})')
                await self._human_like_delay(0.3, 0.8)
                
        except Exception as e:
            logging.warning(f'L·ªói khi simulate human behavior: {e}')
    
    async def _check_login_required(self, page):
        """Ki·ªÉm tra xem trang c√≥ y√™u c·∫ßu ƒëƒÉng nh·∫≠p ho·∫∑c x√°c minh kh√¥ng"""
        try:
            page_url = page.url.lower()
            
            # Ki·ªÉm tra URL cho c√°c trang y√™u c·∫ßu x√°c th·ª±c
            blocked_patterns = [
                '/verify/traffic',  # Trang x√°c minh traffic/captcha
                '/verify/error',
                'login',
                'captcha'
            ]
            
            if any(pattern in page_url for pattern in blocked_patterns):
                return True
                
            return False
        except:
            return False
    
    async def _handle_login(self, page):
        """X·ª≠ l√Ω ƒëƒÉng nh·∫≠p ho·∫∑c x√°c minh captcha"""
        page_url = page.url
        
        if '/verify/traffic' in page_url or '/verify/error' in page_url:
            logging.warning("=" * 70)
            logging.warning("ü§ñ SHOPEE Y√äU C·∫¶U X√ÅC MINH (CAPTCHA/TRAFFIC VERIFICATION)")
            logging.warning("=" * 70)
            logging.warning("Shopee ph√°t hi·ªán ho·∫°t ƒë·ªông b·∫•t th∆∞·ªùng v√† y√™u c·∫ßu x√°c minh.")
            logging.warning("")
            logging.warning("üìã H∆Ø·ªöNG D·∫™N:")
            logging.warning("1. Trong c·ª≠a s·ªï browser, ho√†n th√†nh CAPTCHA ho·∫∑c x√°c minh")
            logging.warning("2. ƒê·ª£i cho ƒë·∫øn khi ƒë∆∞·ª£c chuy·ªÉn v·ªÅ trang b√¨nh th∆∞·ªùng")
            logging.warning("3. Quay l·∫°i console n√†y v√† nh·∫•n Enter")
            logging.warning("=" * 70)
        else:
            logging.warning("=" * 60)
            logging.warning("üîê SHOPEE Y√äU C·∫¶U ƒêƒÇNG NH·∫¨P")
            logging.warning("=" * 60)
            logging.warning("Vui l√≤ng ƒëƒÉng nh·∫≠p v√†o Shopee trong c·ª≠a s·ªï browser.")
            logging.warning("Sau khi ƒëƒÉng nh·∫≠p th√†nh c√¥ng, nh·∫•n Enter ·ªü console n√†y...")
            logging.warning("=" * 60)
            
            # Ch·ªâ chuy·ªÉn v·ªÅ trang ch·ªß n·∫øu kh√¥ng ph·∫£i trang verify
            if 'verify' not in page_url:
                try:
                    await page.goto("https://shopee.vn", wait_until='domcontentloaded', timeout=30000)
                except:
                    pass
        
        # ƒê·ª£i ng∆∞·ªùi d√πng ho√†n th√†nh
        input("\n>>> Nh·∫•n Enter sau khi ho√†n th√†nh: ")
        
        logging.info("‚úÖ Ti·∫øp t·ª•c scraping...")
        await page.wait_for_timeout(3000)
    
    async def scrape(self):
        """H√†m ch√≠nh ƒë·ªÉ scrape d·ªØ li·ªáu"""
        async with async_playwright() as p:
            # S·ª≠ d·ª•ng Firefox ƒë·ªÉ tr√°nh b·ªã ph√°t hi·ªán
            try:
                browser = await p.firefox.launch(
                    headless=self.headless,
                    firefox_user_prefs={
                        'dom.webdriver.enabled': False,
                        'useAutomationExtension': False,
                        'privacy.trackingprotection.enabled': True,
                        'geo.enabled': True,
                        'geo.provider.use_corelocation': True,
                        'geo.prompt.testing': True,
                        'geo.prompt.testing.allow': True
                    }
                )
                logging.info("‚úÖ ƒêang s·ª≠ d·ª•ng Firefox")
            except Exception as e:
                logging.error(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông Firefox: {e}")
                raise
            
            # Ki·ªÉm tra xem c√≥ state ƒë√£ l∆∞u kh√¥ng
            has_saved_state = await self._load_cookies()
            
            if has_saved_state:
                logging.info("T√¨m th·∫•y session ƒë√£ l∆∞u. ƒêang load...")
                context = await browser.new_context(
                    storage_state=self.state_file,
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0',
                    locale='vi-VN',
                    timezone_id='Asia/Ho_Chi_Minh',
                    geolocation={'latitude': 10.8231, 'longitude': 106.6297},
                    permissions=['geolocation']
                )
            else:
                logging.info("Kh√¥ng t√¨m th·∫•y session. T·∫°o session m·ªõi...")
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0',
                    locale='vi-VN',
                    timezone_id='Asia/Ho_Chi_Minh',
                    geolocation={'latitude': 10.8231, 'longitude': 106.6297},
                    permissions=['geolocation']
                )
            
            # Th√™m script ƒë·ªÉ ·∫©n d·∫•u hi·ªáu automation cho Firefox
            await context.add_init_script("""
                // X√≥a webdriver flag
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => false
                });
                
                // Override c√°c thu·ªôc t√≠nh automation
                Object.defineProperty(navigator, 'maxTouchPoints', {
                    get: () => 1
                });
                
                // Gi·∫£ m·∫°o permissions
                const originalQuery = window.navigator.permissions.query;
                window.navigator.permissions.query = (parameters) => (
                    parameters.name === 'notifications' ?
                        Promise.resolve({ state: Notification.permission }) :
                        originalQuery(parameters)
                );
                
                // Override battery API
                if (navigator.getBattery) {
                    navigator.getBattery = () => Promise.resolve({
                        charging: true,
                        chargingTime: 0,
                        dischargingTime: Infinity,
                        level: 1.0,
                        addEventListener: () => {},
                        removeEventListener: () => {}
                    });
                }
            """)
            
            page = await context.new_page()
            
            try:
                # 1. T√¨m ki·∫øm s·∫£n ph·∫©m
                logging.info(f"ƒêang t√¨m ki·∫øm: {self.search_term}")
                products = await self._search_products(page)
                
                # Ki·ªÉm tra n·∫øu c·∫ßn ƒëƒÉng nh·∫≠p ho·∫∑c x√°c minh
                max_retries = 3
                retry_count = 0
                
                while await self._check_login_required(page) and retry_count < max_retries:
                    await self._handle_login(page)
                    # Th·ª≠ t√¨m ki·∫øm l·∫°i sau khi x√°c minh/ƒëƒÉng nh·∫≠p
                    products = await self._search_products(page)
                    retry_count += 1
                    
                    if not await self._check_login_required(page):
                        break
                    
                    if retry_count >= max_retries:
                        logging.error("‚ùå V·∫´n kh√¥ng th·ªÉ truy c·∫≠p sau nhi·ªÅu l·∫ßn th·ª≠. Vui l√≤ng th·ª≠ l·∫°i sau.")
                        return
                
                if not products:
                    logging.error("Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o!")
                    return
                
                # L∆∞u session sau khi x√°c nh·∫≠n c√≥ th·ªÉ truy c·∫≠p
                await self._save_cookies(context)
                
                # 2. L·∫•y chi ti·∫øt v√† reviews cho t·ª´ng s·∫£n ph·∫©m
                logging.info(f"T√¨m th·∫•y {len(products)} s·∫£n ph·∫©m. ƒêang l·∫•y chi ti·∫øt...")
                for idx, product in enumerate(tqdm(products, desc="Scraping products")):
                    try:
                        # Delay ng·∫´u nhi√™n gi·ªØa c√°c s·∫£n ph·∫©m (quan tr·ªçng!)
                        if idx > 0:
                            await self._human_like_delay(3, 7)
                        
                        await self._scrape_product_details(page, product)
                        self.products_data.append(product)
                        
                        # L∆∞u session v√† data ƒë·ªãnh k·ª≥
                        if (idx + 1) % 5 == 0:
                            self._save_data()
                            await self._save_cookies(context)
                    except Exception as e:
                        logging.error(f"L·ªói khi scrape s·∫£n ph·∫©m {product.get('name', 'Unknown')}: {e}")
                        continue
                
                # L∆∞u l·∫ßn cu·ªëi
                self._save_data()
                await self._save_cookies(context)
                logging.info(f"Ho√†n th√†nh! D·ªØ li·ªáu ƒë√£ l∆∞u v√†o {self.output_file}")
                
            finally:
                await browser.close()
    
    async def _search_products(self, page):
        """T√¨m ki·∫øm v√† l·∫•y danh s√°ch s·∫£n ph·∫©m"""
        search_url = f"https://shopee.vn/search?keyword={self.search_term.replace(' ', '%20')}&sortBy=sales"
        
        try:
            await page.goto(search_url, wait_until='networkidle', timeout=60000)
        except:
            await page.goto(search_url, wait_until='domcontentloaded', timeout=60000)
        
        # Delay t·ª± nhi√™n sau khi load trang
        await self._human_like_delay(2, 4)
        
        # Simulate h√†nh vi ng∆∞·ªùi d√πng
        await self._simulate_human_behavior(page)
        
        # Ki·ªÉm tra l·∫°i n·∫øu b·ªã redirect v·ªÅ trang ƒëƒÉng nh·∫≠p
        if await self._check_login_required(page):
            logging.warning("Trang y√™u c·∫ßu ƒëƒÉng nh·∫≠p!")
            return []
        
        products = []
        try:
            # ƒê·ª£i container s·∫£n ph·∫©m xu·∫•t hi·ªán
            await page.wait_for_selector('.shopee-search-item-result__items', timeout=10000)
            
            # L·∫•y th√¥ng tin c√°c s·∫£n ph·∫©m
            product_cards = await page.query_selector_all('.shopee-search-item-result__item')
            
            for card in product_cards[:self.max_products]:
                try:
                    product = {}
                    
                    # Link s·∫£n ph·∫©m
                    link_elem = await card.query_selector('a')
                    if link_elem:
                        href = await link_elem.get_attribute('href')
                        product['link'] = f"https://shopee.vn{href}" if href.startswith('/') else href
                    
                    # T√™n s·∫£n ph·∫©m
                    name_elem = await card.query_selector('[data-sqe="name"]')
                    if name_elem:
                        product['name'] = await name_elem.inner_text()
                    
                    # Gi√°
                    price_elem = await card.query_selector('.fxMUzH, .JRplV8')
                    if price_elem:
                        product['price'] = await price_elem.inner_text()
                    
                    # Rating
                    rating_elem = await card.query_selector('.rES4jh')
                    if rating_elem:
                        product['rating'] = await rating_elem.inner_text()
                    
                    # S·ªë l∆∞·ª£ng ƒë√£ b√°n
                    sold_elem = await card.query_selector('.CTxYvB')
                    if sold_elem:
                        product['sold'] = await sold_elem.inner_text()
                    
                    # H√¨nh ·∫£nh
                    img_elem = await card.query_selector('img')
                    if img_elem:
                        product['image'] = await img_elem.get_attribute('src')
                    
                    # Location
                    location_elem = await card.query_selector('.mAKokq')
                    if location_elem:
                        product['location'] = await location_elem.inner_text()
                    
                    if product.get('link'):
                        products.append(product)
                        
                except Exception as e:
                    logging.warning(f"L·ªói khi parse s·∫£n ph·∫©m: {e}")
                    continue
            
        except Exception as e:
            logging.error(f"L·ªói khi t√¨m ki·∫øm s·∫£n ph·∫©m: {e}")
        
        return products
    
    async def _scrape_product_details(self, page, product):
        """L·∫•y chi ti·∫øt s·∫£n ph·∫©m v√† reviews"""
        try:
            await page.goto(product['link'], wait_until='networkidle', timeout=60000)
            
            # Delay t·ª± nhi√™n
            await self._human_like_delay(2, 4)
            
            # Ki·ªÉm tra n·∫øu g·∫∑p trang x√°c minh
            if await self._check_login_required(page):
                logging.warning(f"‚ö†Ô∏è G·∫∑p trang x√°c minh khi truy c·∫≠p {product.get('name', 'Unknown')}")
                await self._handle_login(page)
                # Th·ª≠ load l·∫°i trang s·∫£n ph·∫©m
                await page.goto(product['link'], wait_until='domcontentloaded', timeout=60000)
                await self._human_like_delay(2, 3)
            
            # Simulate h√†nh vi xem s·∫£n ph·∫©m
            await self._simulate_human_behavior(page)
            
            # L·∫•y m√¥ t·∫£
            try:
                desc_elem = await page.query_selector('.nq5KNw, [data-sqe="description"]')
                if desc_elem:
                    product['description'] = await desc_elem.inner_text()
            except:
                product['description'] = ""
            
            # L·∫•y category
            try:
                breadcrumb = await page.query_selector('.breadcrumb')
                if breadcrumb:
                    product['category'] = await breadcrumb.inner_text()
            except:
                product['category'] = ""
            
            # Scroll xu·ªëng ph·∫ßn reviews
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await page.wait_for_timeout(2000)
            
            # L·∫•y reviews
            product['reviews'] = await self._scrape_reviews(page)
            
        except Exception as e:
            logging.error(f"L·ªói khi l·∫•y chi ti·∫øt s·∫£n ph·∫©m: {e}")
            product['description'] = ""
            product['category'] = ""
            product['reviews'] = []
    
    async def _scrape_reviews(self, page):
        """L·∫•y reviews c·ªßa s·∫£n ph·∫©m"""
        reviews = []
        try:
            # ƒê·ª£i ph·∫ßn reviews xu·∫•t hi·ªán
            try:
                await page.wait_for_selector('.product-ratings__list, .shopee-product-rating', timeout=5000)
            except PlaywrightTimeout:
                logging.info("Kh√¥ng t√¨m th·∫•y reviews cho s·∫£n ph·∫©m n√†y")
                return reviews
            
            review_count = 0
            page_num = 1
            max_pages = (self.max_reviews // 6) + 1  # M·ªói trang th∆∞·ªùng c√≥ ~6 reviews
            
            while review_count < self.max_reviews and page_num <= max_pages:
                # L·∫•y reviews trong trang hi·ªán t·∫°i
                review_items = await page.query_selector_all('.shopee-product-rating__main, .product-rating-item')
                
                for item in review_items:
                    if review_count >= self.max_reviews:
                        break
                    
                    try:
                        review = {}
                        
                        # T√™n ng∆∞·ªùi ƒë√°nh gi√°
                        author_elem = await item.query_selector('.shopee-product-rating__author-name, .author-name')
                        if author_elem:
                            review['author'] = await author_elem.inner_text()
                        
                        # Rating (s·ªë sao)
                        stars_elem = await item.query_selector('.shopee-product-rating__rating, .rating-stars')
                        if stars_elem:
                            stars_html = await stars_elem.inner_html()
                            review['rating'] = stars_html.count('icon-rating-solid--active')
                        
                        # Th·ªùi gian
                        time_elem = await item.query_selector('.shopee-product-rating__time, .time')
                        if time_elem:
                            review['time'] = await time_elem.inner_text()
                        
                        # N·ªôi dung review
                        content_elem = await item.query_selector('.shopee-product-rating__content, .review-content')
                        if content_elem:
                            review['content'] = await content_elem.inner_text()
                        
                        # Ph·∫£n h·ªìi t·ª´ shop
                        seller_reply_elem = await item.query_selector('.shopee-product-rating__shop-reply, .seller-reply')
                        if seller_reply_elem:
                            review['seller_reply'] = await seller_reply_elem.inner_text()
                        else:
                            review['seller_reply'] = ""
                        
                        # S·ªë l∆∞·ª£t th√≠ch
                        like_elem = await item.query_selector('.shopee-product-rating__like-count, .like-count')
                        if like_elem:
                            like_text = await like_elem.inner_text()
                            review['likes'] = like_text
                        else:
                            review['likes'] = "0"
                        
                        reviews.append(review)
                        review_count += 1
                        
                    except Exception as e:
                        logging.warning(f"L·ªói khi parse review: {e}")
                        continue
                
                # Th·ª≠ click n√∫t next page
                if review_count < self.max_reviews:
                    try:
                        next_button = await page.query_selector('.shopee-icon-button--right, .product-rating-overview__page-next')
                        if next_button:
                            is_disabled = await next_button.get_attribute('disabled')
                            if not is_disabled:
                                await next_button.click()
                                await page.wait_for_timeout(2000)
                                page_num += 1
                            else:
                                break
                        else:
                            break
                    except:
                        break
                else:
                    break
                    
        except Exception as e:
            logging.error(f"L·ªói khi l·∫•y reviews: {e}")
        
        return reviews
    
    def _save_data(self):
        """L∆∞u d·ªØ li·ªáu v√†o file JSON"""
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(self.products_data, f, ensure_ascii=False, indent=2)
            logging.info(f"ƒê√£ l∆∞u {len(self.products_data)} s·∫£n ph·∫©m v√†o {self.output_file}")
        except Exception as e:
            logging.error(f"L·ªói khi l∆∞u file: {e}")


async def main():
    parser = argparse.ArgumentParser(description='Shopee Scraper s·ª≠ d·ª•ng Playwright')
    parser.add_argument('-k', '--keyword', default='Raspberry pi', help='T·ª´ kh√≥a t√¨m ki·∫øm')
    parser.add_argument('-n', '--num', type=int, default=10, help='S·ªë l∆∞·ª£ng s·∫£n ph·∫©m')
    parser.add_argument('-r', '--reviews', type=int, default=30, help='S·ªë l∆∞·ª£ng reviews t·ªëi ƒëa m·ªói s·∫£n ph·∫©m')
    parser.add_argument('--headless', action='store_true', help='Ch·∫°y browser ·∫©n')
    
    args = parser.parse_args()
    
    scraper = ShopeePlaywrightScraper(
        search_term=args.keyword,
        max_products=args.num,
        max_reviews=args.reviews,
        headless=args.headless
    )
    
    await scraper.scrape()


if __name__ == "__main__":
    asyncio.run(main())