import asyncio
import json
import logging
import argparse
import re
import random
from pathlib import Path
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout
from tqdm.asyncio import tqdm
import aiohttp

# Setup logging
import sys
if sys.stdout.encoding != 'utf-8':
    sys.stdout.reconfigure(encoding='utf-8')
if sys.stderr.encoding != 'utf-8':
    sys.stderr.reconfigure(encoding='utf-8')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('tiki_scraper.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)

class TikiPlaywrightScraper:
    def __init__(self, search_term, max_products=10, max_reviews=30, headless=False, max_concurrent=10):
        """
        Scraper s·ª≠ d·ª•ng Playwright ƒë·ªÉ l·∫•y d·ªØ li·ªáu t·ª´ Tiki.
        
        Args:
            search_term: T·ª´ kh√≥a t√¨m ki·∫øm
            max_products: S·ªë l∆∞·ª£ng s·∫£n ph·∫©m t·ªëi ƒëa
            max_reviews: S·ªë l∆∞·ª£ng review t·ªëi ƒëa cho m·ªói s·∫£n ph·∫©m
            headless: Ch·∫°y browser ·∫©n hay kh√¥ng
            max_concurrent: S·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi t·ªëi ƒëa (m·∫∑c ƒë·ªãnh: 5)
        """
        self.search_term = search_term
        self.max_products = max_products
        self.max_reviews = max_reviews
        self.headless = headless
        self.max_concurrent = max_concurrent
        self.output_file = f"tiki_product.json"
        self.products_data = []
        self.state_file = "tiki_state.json"
        self.semaphore = None  # S·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o trong async context
        
    async def _save_cookies(self, context):
        """L∆∞u cookies v√† storage state ƒë·ªÉ duy tr√¨ session"""
        try:
            await context.storage_state(path=self.state_file)
            logging.info(f"ƒê√£ l∆∞u session v√†o {self.state_file}")
        except Exception as e:
            logging.warning(f"Kh√¥ng th·ªÉ l∆∞u session: {e}")
    
    async def _load_cookies(self):
        """Ki·ªÉm tra xem c√≥ file state ƒë√£ l∆∞u kh√¥ng"""
        return Path(self.state_file).exists()
    
    async def _human_like_delay(self, min_sec=1, max_sec=3):
        """T·∫°o delay ng·∫´u nhi√™n gi·ªëng ng∆∞·ªùi d√πng th·∫≠t"""
        delay = random.uniform(min_sec, max_sec)
        await asyncio.sleep(delay)
    
    async def _simulate_human_behavior(self, page):
        """M√¥ ph·ªèng h√†nh vi ng∆∞·ªùi d√πng: di chuy·ªÉn chu·ªôt, scroll t·ª± nhi√™n"""
        try:
            # Random mouse movements
            for _ in range(random.randint(2, 4)):
                x = random.randint(100, 1500)
                y = random.randint(100, 800)
                await page.mouse.move(x, y)
                await self._human_like_delay(0.2, 0.5)
            
            # Scroll t·ª± nhi√™n t·ª´ng ƒëo·∫°n nh·ªè
            scroll_steps = random.randint(3, 5)
            for _ in range(scroll_steps):
                scroll_amount = random.randint(200, 500)
                await page.evaluate(f'window.scrollBy({{top: {scroll_amount}, behavior: "smooth"}})')
                await self._human_like_delay(0.5, 1.2)
            
            # Scroll back l√™n m·ªôt ch√∫t
            if random.random() > 0.5:
                await page.evaluate(f'window.scrollBy({{top: -{random.randint(100, 300)}, behavior: "smooth"}})')
                await self._human_like_delay(0.3, 0.8)
                
        except Exception as e:
            logging.warning(f'L·ªói khi simulate human behavior: {e}')
    
    async def scrape(self):
        """H√†m ch√≠nh ƒë·ªÉ scrape d·ªØ li·ªáu v·ªõi x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô song song"""
        # Kh·ªüi t·∫°o semaphore ƒë·ªÉ gi·ªõi h·∫°n s·ªë request ƒë·ªìng th·ªùi
        self.semaphore = asyncio.Semaphore(self.max_concurrent)
        
        async with async_playwright() as p:
            try:
                browser = await p.firefox.launch(
                    headless=self.headless,
                    firefox_user_prefs={
                        'dom.webdriver.enabled': False,
                        'useAutomationExtension': False,
                    }
                )
                logging.info("‚úÖ ƒêang s·ª≠ d·ª•ng Firefox")
            except Exception as e:
                logging.error(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông Firefox: {e}")
                raise
            
            # Load state n·∫øu c√≥
            has_saved_state = await self._load_cookies()
            
            if has_saved_state:
                logging.info("T√¨m th·∫•y session ƒë√£ l∆∞u. ƒêang load...")
                context = await browser.new_context(
                    storage_state=self.state_file,
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0',
                    locale='vi-VN',
                    timezone_id='Asia/Ho_Chi_Minh'
                )
            else:
                logging.info("Kh√¥ng t√¨m th·∫•y session. T·∫°o session m·ªõi...")
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0',
                    locale='vi-VN',
                    timezone_id='Asia/Ho_Chi_Minh'
                )
            
            # Anti-detection script
            await context.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => false
                });
            """)
            
            page = await context.new_page()
            
            try:
                # 1. T√¨m ki·∫øm s·∫£n ph·∫©m qua API
                logging.info(f"üîç ƒêang t√¨m ki·∫øm: {self.search_term}")
                logging.info("üì° S·ª≠ d·ª•ng Tiki API ƒë·ªÉ t√¨m ki·∫øm...")
                products = await self._search_products(page)
                
                if not products:
                    logging.error("‚ùå Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m n√†o!")
                    return
                
                logging.info(f"‚úÖ T√¨m th·∫•y {len(products)} s·∫£n ph·∫©m")
                
                await self._save_cookies(context)
                
                # 2. L·∫•y chi ti·∫øt v√† reviews cho t·ª´ng s·∫£n ph·∫©m SONG SONG
                logging.info(f"üöÄ ƒêang l·∫•y chi ti·∫øt {len(products)} s·∫£n ph·∫©m (song song {self.max_concurrent} requests)...")
                
                # T·∫°o tasks cho t·∫•t c·∫£ s·∫£n ph·∫©m
                tasks = []
                for product in products:
                    task = self._scrape_product_with_semaphore(page, product)
                    tasks.append(task)
                
                # Ch·∫°y t·∫•t c·∫£ tasks song song v·ªõi progress bar
                results = []
                for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="Scraping products"):
                    try:
                        result = await coro
                        if result:
                            results.append(result)
                            self.products_data.append(result)
                            
                            # L∆∞u ƒë·ªãnh k·ª≥ m·ªói 5 s·∫£n ph·∫©m
                            if len(results) % 5 == 0:
                                self._save_data()
                                await self._save_cookies(context)
                    except Exception as e:
                        logging.error(f"L·ªói khi x·ª≠ l√Ω task: {e}")
                        continue
                
                # L∆∞u l·∫ßn cu·ªëi
                self._save_data()
                logging.info(f"‚úÖ Ho√†n th√†nh! ƒê√£ l·∫•y ƒë∆∞·ª£c {len(self.products_data)} s·∫£n ph·∫©m")
                
                # return last value for calling function
                return self.products_data
                
            finally:
                await browser.close()
    
    async def _scrape_product_with_semaphore(self, page, product):
        """Wrapper ƒë·ªÉ scrape product v·ªõi semaphore control"""
        async with self.semaphore:
            try:
                # Random delay nh·ªè ƒë·ªÉ tr√°nh spam
                await self._human_like_delay(0.5, 1.5)
                
                # L·∫•y chi ti·∫øt s·∫£n ph·∫©m (ƒë√£ optimize ƒë·ªÉ d√πng API)
                await self._scrape_product_details(page, product)
                return product
            except Exception as e:
                logging.error(f"L·ªói khi scrape s·∫£n ph·∫©m {product.get('name', 'Unknown')}: {e}")
                return None
    
    async def _search_products_api(self):
        """T√¨m ki·∫øm s·∫£n ph·∫©m qua Tiki API - nhanh v√† ·ªïn ƒë·ªãnh h∆°n v·ªõi timeout v√† retry"""
        products = []
        
        # API endpoint c·ªßa Tiki
        api_url = "https://tiki.vn/api/v2/products"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Referer': f'https://tiki.vn/search?q={self.search_term.replace(" ", "+")}',
            'x-guest-token': 'default'
        }
        
        params = {
            'limit': min(self.max_products, 40),  # Tiki gi·ªõi h·∫°n 40/request
            'include': 'advertisement',
            'aggregations': '2',
            'q': self.search_term
        }
        
        # Th·ª≠ t·ªëi ƒëa 3 l·∫ßn n·∫øu fail
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # S·ª≠ d·ª•ng timeout ƒë·ªÉ tr√°nh treo
                timeout = aiohttp.ClientTimeout(total=30)
                async with aiohttp.ClientSession(timeout=timeout) as session:
                    async with session.get(api_url, params=params, headers=headers) as response:
                        if response.status == 200:
                            data = await response.json()
                            
                            # Parse d·ªØ li·ªáu t·ª´ API
                            items = data.get('data', [])
                            
                            for item in items[:self.max_products]:
                                try:
                                    product = {
                                        'id': item.get('id'),
                                        'name': item.get('name', ''),
                                        'link': f"https://tiki.vn/{item.get('url_path', '')}" if item.get('url_path') else f"https://tiki.vn/product-p{item.get('id')}.html",
                                        'price': item.get('price', 0),
                                        'original_price': item.get('original_price', 0),
                                        'discount': item.get('discount_rate', 0),
                                        'rating': item.get('rating_average', 0),
                                        'review_count': item.get('review_count', 0),
                                        'quantity_sold': item.get('quantity_sold', {}).get('value', 0),
                                        'image': item.get('thumbnail_url', ''),
                                        'badges': item.get('badges_new', []),
                                        'seller': item.get('seller', {}).get('name', ''),
                                        'brand': item.get('brand_name', ''),
                                        'specifications': item.get('specifications', [])
                                    }
                                    
                                    products.append(product)
                                    logging.info(f"‚úÖ T√¨m th·∫•y: {product['name'][:50]}...")
                                    
                                except Exception as e:
                                    logging.warning(f"L·ªói khi parse s·∫£n ph·∫©m: {e}")
                                    continue
                            
                            return products  # Th√†nh c√¥ng, return ngay
                        else:
                            logging.warning(f"API tr·∫£ v·ªÅ status {response.status}, th·ª≠ l·∫°i l·∫ßn {attempt + 1}/{max_retries}")
                            
            except asyncio.TimeoutError:
                logging.warning(f"Timeout khi g·ªçi API, th·ª≠ l·∫°i l·∫ßn {attempt + 1}/{max_retries}")
            except Exception as e:
                logging.error(f"L·ªói khi g·ªçi API (l·∫ßn {attempt + 1}/{max_retries}): {e}")
            
            # Ch·ªù tr∆∞·ªõc khi retry
            if attempt < max_retries - 1:
                await asyncio.sleep(2 ** attempt)  # Exponential backoff
        
        return products
    
    async def _search_products(self, page):
        """T√¨m ki·∫øm s·∫£n ph·∫©m - s·ª≠ d·ª•ng API tr∆∞·ªõc, fallback v·ªÅ scraping n·∫øu c·∫ßn"""
        # Th·ª≠ API tr∆∞·ªõc (nhanh h∆°n)
        products = await self._search_products_api()
        
        if products:
            return products
        
        # Fallback: scrape HTML n·∫øu API fail
        logging.warning("API kh√¥ng ho·∫°t ƒë·ªông, chuy·ªÉn sang scrape HTML...")
        search_url = f"https://tiki.vn/search?q={self.search_term.replace(' ', '+')}"
        
        try:
            await page.goto(search_url, wait_until='networkidle', timeout=60000)
        except:
            await page.goto(search_url, wait_until='domcontentloaded', timeout=60000)
        
        await self._human_like_delay(2, 4)
        
        products = []
        try:
            await page.wait_for_selector('.product-item', timeout=10000)
            product_items = await page.query_selector_all('.product-item')
            
            for item in product_items[:self.max_products]:
                try:
                    product = {}
                    
                    link_elem = await item.query_selector('a')
                    if link_elem:
                        href = await link_elem.get_attribute('href')
                        product['link'] = f"https://tiki.vn{href}" if not href.startswith('http') else href
                    
                    name_elem = await item.query_selector('.name')
                    if name_elem:
                        product['name'] = (await name_elem.inner_text()).strip()
                    
                    price_elem = await item.query_selector('.price-discount__price')
                    if price_elem:
                        product['price'] = (await price_elem.inner_text()).strip()
                    
                    if product.get('link'):
                        products.append(product)
                        logging.info(f"T√¨m th·∫•y: {product.get('name', 'Unknown')}")
                        
                except Exception as e:
                    logging.warning(f"L·ªói khi parse s·∫£n ph·∫©m: {e}")
                    continue
            
        except Exception as e:
            logging.error(f"L·ªói khi scrape: {e}")
        
        return products
    
    async def _get_product_details_api(self, product_id, session):
        """L·∫•y chi ti·∫øt s·∫£n ph·∫©m qua API - s·ª≠ d·ª•ng session ƒë∆∞·ª£c chia s·∫ª ƒë·ªÉ t√°i s·ª≠ d·ª•ng connection"""
        api_url = f"https://tiki.vn/api/v2/products/{product_id}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0',
            'Accept': 'application/json, text/plain, */*',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Referer': f'https://tiki.vn/product-p{product_id}.html',
            'x-guest-token': 'default'
        }
        
        params = {
            'platform': 'web',
            'version': '3'
        }
        
        try:
            async with session.get(api_url, params=params, headers=headers) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    details = {
                        'description': data.get('description', ''),
                        'short_description': data.get('short_description', ''),
                        'specifications': [],
                        'brand': {},
                        'categories': data.get('categories', {}),
                        'images': data.get('images', []),
                        'current_seller': data.get('current_seller', {}),
                        'stock_item': data.get('stock_item', {}),
                        'warranty_info': data.get('warranty_info', ''),
                        'return_and_exchange_policy': data.get('return_and_exchange_policy', '')
                    }
                    
                    # Parse specifications
                    specs_list = data.get('specifications', [])
                    if specs_list:
                        for spec_group in specs_list:
                            attributes = spec_group.get('attributes', [])
                            for attr in attributes:
                                details['specifications'].append({
                                    'name': attr.get('name', ''),
                                    'value': attr.get('value', '')
                                })
                    
                    # Brand info
                    brand_data = data.get('brand', {})
                    if brand_data:
                        details['brand'] = {
                            'id': brand_data.get('id'),
                            'name': brand_data.get('name', '')
                        }
                    
                    return details
                else:
                    logging.warning(f"API tr·∫£ v·ªÅ status {response.status} cho product {product_id}")
                    return None
                    
        except Exception as e:
            logging.error(f"L·ªói khi g·ªçi API chi ti·∫øt s·∫£n ph·∫©m {product_id}: {e}")
            return None
    
    async def _scrape_product_details(self, page, product):
        """L·∫•y chi ti·∫øt s·∫£n ph·∫©m - ∆∞u ti√™n API song song, fallback HTML n·∫øu c·∫ßn"""
        product_id = product.get('id')
        
        if product_id:
            # Th·ª≠ l·∫•y t·ª´ API tr∆∞·ªõc v·ªõi shared session
            logging.info(f"üì° L·∫•y chi ti·∫øt qua API cho product {product_id}...")
            
            # T·∫°o session d√πng chung cho c·∫£ details v√† reviews ƒë·ªÉ t√°i s·ª≠ d·ª•ng connection
            timeout = aiohttp.ClientTimeout(total=30)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                # L·∫•y details v√† reviews SONG SONG
                details_task = self._get_product_details_api(product_id, session)
                reviews_task = self._get_reviews_api(product_id, session)
                
                # Ch·ªù c·∫£ 2 tasks ho√†n th√†nh ƒë·ªìng th·ªùi
                details, reviews = await asyncio.gather(
                    details_task,
                    reviews_task,
                    return_exceptions=True
                )
                
                # X·ª≠ l√Ω k·∫øt qu·∫£ details
                if isinstance(details, Exception):
                    logging.error(f"L·ªói khi l·∫•y details: {details}")
                    details = None
                
                # X·ª≠ l√Ω k·∫øt qu·∫£ reviews
                if isinstance(reviews, Exception):
                    logging.error(f"L·ªói khi l·∫•y reviews: {reviews}")
                    reviews = []
                
                if details:
                    # Merge details v√†o product
                    product.update(details)
                    product['reviews'] = reviews if reviews else []
                    return
        
        # Fallback: Scrape HTML n·∫øu API fail ho·∫∑c kh√¥ng c√≥ product_id
        logging.warning(f"‚ö†Ô∏è API kh√¥ng ho·∫°t ƒë·ªông, scrape HTML cho {product.get('name', 'Unknown')[:50]}...")
        
        try:
            await page.goto(product['link'], wait_until='networkidle', timeout=60000)
            await self._human_like_delay(2, 4)
            
            # L·∫•y m√¥ t·∫£
            try:
                desc_elem = await page.query_selector('.ToggleContent__Wrapper-sc-fbuwol-0, .content')
                if desc_elem:
                    product['description'] = (await desc_elem.inner_text()).strip()
            except:
                product['description'] = ""
            
            # L·∫•y th√¥ng s·ªë k·ªπ thu·∫≠t
            try:
                specs = []
                spec_rows = await page.query_selector_all('.ProductInfo__TableSpecs-sc-1j7z4jf-0 tr')
                for row in spec_rows:
                    cells = await row.query_selector_all('td')
                    if len(cells) >= 2:
                        specs.append({
                            'name': (await cells[0].inner_text()).strip(),
                            'value': (await cells[1].inner_text()).strip()
                        })
                product['specifications'] = specs
            except:
                product['specifications'] = []
            
            # Scroll xu·ªëng ph·∫ßn reviews
            await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
            await self._human_like_delay(1, 2)
            
            # L·∫•y reviews t·ª´ HTML
            product['reviews'] = await self._scrape_reviews(page)
            
        except Exception as e:
            logging.error(f"L·ªói khi l·∫•y chi ti·∫øt s·∫£n ph·∫©m: {e}")
            product['description'] = ""
            product['specifications'] = []
            product['reviews'] = []
    
    async def _get_reviews_api(self, product_id, session):
        """L·∫•y reviews qua API - s·ª≠ d·ª•ng session ƒë∆∞·ª£c chia s·∫ª"""
        reviews = []
        api_url = f"https://tiki.vn/api/v2/reviews"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:133.0) Gecko/20100101 Firefox/133.0',
            'Accept': 'application/json, text/plain, */*',
            'Referer': f'https://tiki.vn/product-p{product_id}.html',
            'x-guest-token': 'default'
        }
        
        # N·∫øu c·∫ßn l·∫•y nhi·ªÅu trang reviews, t√≠nh s·ªë trang
        pages_needed = (self.max_reviews + 19) // 20  # L√†m tr√≤n l√™n
        pages_needed = min(pages_needed, 5)  # Gi·ªõi h·∫°n t·ªëi ƒëa 5 trang ƒë·ªÉ tr√°nh qu√° l√¢u
        
        try:
            # N·∫øu c·∫ßn nhi·ªÅu trang, l·∫•y song song
            if pages_needed > 1:
                tasks = []
                for page_num in range(1, pages_needed + 1):
                    params = {
                        'product_id': product_id,
                        'limit': 20,
                        'sort': 'score|desc,id|desc,stars|all',
                        'page': page_num
                    }
                    task = session.get(api_url, params=params, headers=headers)
                    tasks.append(task)
                
                # L·∫•y t·∫•t c·∫£ c√°c trang song song
                responses = await asyncio.gather(*tasks, return_exceptions=True)
                
                for response in responses:
                    if isinstance(response, Exception):
                        logging.warning(f"L·ªói khi l·∫•y review page: {response}")
                        continue
                    
                    async with response:
                        if response.status == 200:
                            data = await response.json()
                            review_data = data.get('data', [])
                            
                            for review_item in review_data:
                                if len(reviews) >= self.max_reviews:
                                    break
                                try:
                                    review = {
                                        'id': review_item.get('id'),
                                        'title': review_item.get('title', ''),
                                        'content': review_item.get('content', ''),
                                        'rating': review_item.get('rating', 0),
                                        'author': review_item.get('created_by', {}).get('name', 'Anonymous'),
                                        'time': review_item.get('created_at', ''),
                                        'helpful_count': review_item.get('thank_count', 0),
                                        'images': review_item.get('images', []),
                                        'timeline': review_item.get('timeline', {}),
                                        'customer_reviewed': review_item.get('customer_reviewed', {})
                                    }
                                    reviews.append(review)
                                except Exception as e:
                                    logging.warning(f"L·ªói parse review: {e}")
                                    continue
            else:
                # Ch·ªâ c·∫ßn 1 trang
                params = {
                    'product_id': product_id,
                    'limit': min(self.max_reviews, 20),
                    'sort': 'score|desc,id|desc,stars|all',
                    'page': 1
                }
                
                async with session.get(api_url, params=params, headers=headers) as response:
                    if response.status == 200:
                        data = await response.json()
                        review_data = data.get('data', [])
                        
                        for review_item in review_data[:self.max_reviews]:
                            try:
                                review = {
                                    'id': review_item.get('id'),
                                    'title': review_item.get('title', ''),
                                    'content': review_item.get('content', ''),
                                    'rating': review_item.get('rating', 0),
                                    'author': review_item.get('created_by', {}).get('name', 'Anonymous'),
                                    'time': review_item.get('created_at', ''),
                                    'helpful_count': review_item.get('thank_count', 0),
                                    'images': review_item.get('images', []),
                                    'timeline': review_item.get('timeline', {}),
                                    'customer_reviewed': review_item.get('customer_reviewed', {})
                                }
                                reviews.append(review)
                            except Exception as e:
                                logging.warning(f"L·ªói parse review: {e}")
                                continue
                    else:
                        logging.warning(f"Review API tr·∫£ v·ªÅ status {response.status}")
            
            logging.info(f"‚úÖ L·∫•y ƒë∆∞·ª£c {len(reviews)} reviews t·ª´ API cho product {product_id}")
                        
        except Exception as e:
            logging.error(f"L·ªói khi l·∫•y reviews API cho product {product_id}: {e}")
        
        return reviews
    
    async def _scrape_reviews(self, page):
        """L·∫•y reviews t·ª´ HTML (fallback)"""
        reviews = []
        try:
            # Click v√†o tab ƒë√°nh gi√°
            try:
                review_tab = await page.query_selector('div[role="tab"]:has-text("ƒê√°nh gi√°")')
                if review_tab:
                    await review_tab.click()
                    await self._human_like_delay(1, 2)
            except:
                pass
            
            # ƒê·ª£i reviews load
            try:
                await page.wait_for_selector('[data-view-id="pdp_review_list"]', timeout=5000)
            except PlaywrightTimeout:
                logging.info("Kh√¥ng t√¨m th·∫•y reviews cho s·∫£n ph·∫©m n√†y")
                return reviews
            
            review_count = 0
            review_items = await page.query_selector_all('[data-view-id="pdp_review_item"]')
            
            for item in review_items:
                if review_count >= self.max_reviews:
                    break
                
                try:
                    review = {}
                    
                    author_elem = await item.query_selector('[data-view-id="pdp_review_author"]')
                    if author_elem:
                        review['author'] = (await author_elem.inner_text()).strip()
                    
                    rating_elem = await item.query_selector('[data-view-id="pdp_review_rating"]')
                    if rating_elem:
                        stars = await rating_elem.query_selector_all('svg')
                        review['rating'] = len(stars)
                    
                    time_elem = await item.query_selector('[data-view-id="pdp_review_time"]')
                    if time_elem:
                        review['time'] = (await time_elem.inner_text()).strip()
                    
                    content_elem = await item.query_selector('[data-view-id="pdp_review_content"]')
                    if content_elem:
                        review['content'] = (await content_elem.inner_text()).strip()
                    
                    helpful_elem = await item.query_selector('[data-view-id="pdp_review_helpful"]')
                    if helpful_elem:
                        review['helpful'] = (await helpful_elem.inner_text()).strip()
                    
                    reviews.append(review)
                    review_count += 1
                    
                except Exception as e:
                    logging.warning(f"L·ªói khi parse review: {e}")
                    continue
                    
        except Exception as e:
            logging.error(f"L·ªói khi l·∫•y reviews: {e}")
        
        return reviews
    
    def _save_data(self):
        """L∆∞u d·ªØ li·ªáu v√†o file JSON"""
        try:
            with open(self.output_file, 'a', encoding='utf-8') as f:
                json.dump(self.products_data, f, ensure_ascii=False, indent=2)
            logging.info(f"ƒê√£ l∆∞u {len(self.products_data)} s·∫£n ph·∫©m v√†o {self.output_file}")
        except Exception as e:
            logging.error(f"L·ªói khi l∆∞u file: {e}")


async def main():
    parser = argparse.ArgumentParser(description='Tiki Scraper s·ª≠ d·ª•ng Playwright v·ªõi x·ª≠ l√Ω b·∫•t ƒë·ªìng b·ªô')
    parser.add_argument('-k', '--keyword', default='iPhone', help='T·ª´ kh√≥a t√¨m ki·∫øm')
    parser.add_argument('-n', '--num', type=int, default=10, help='S·ªë l∆∞·ª£ng s·∫£n ph·∫©m')
    parser.add_argument('-r', '--reviews', type=int, default=20, help='S·ªë l∆∞·ª£ng reviews t·ªëi ƒëa m·ªói s·∫£n ph·∫©m')
    parser.add_argument('-c', '--concurrent', type=int, default=5, help='S·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi (m·∫∑c ƒë·ªãnh: 5)')
    parser.add_argument('--headless', action='store_true', help='Ch·∫°y browser ·∫©n')
    
    args = parser.parse_args()
    
    scraper = TikiPlaywrightScraper(
        search_term=args.keyword,
        max_products=args.num,
        max_reviews=args.reviews,
        headless=args.headless,
        max_concurrent=args.concurrent
    )
    
    await scraper.scrape()


if __name__ == "__main__":
    asyncio.run(main())
